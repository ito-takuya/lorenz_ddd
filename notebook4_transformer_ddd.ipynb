{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75a7e11a",
   "metadata": {},
   "source": [
    "# Tutorial 4: Data-driven discovery of Lorenz system: Transformers\n",
    "\n",
    "#### Author: Taku Ito\n",
    "\n",
    "7/7/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636bacfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tutorial_ddd\n",
    "import tutorial_ddd.lorenz\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44958e55",
   "metadata": {},
   "source": [
    "#### Experiment: We want to infer the governing differential equations (e.g., $\\dot{x}$, $\\dot{y}$, $\\dot{z}$) from $x$, $y$, and $z$ using a transformer\n",
    "* So, we want to infer the derivatives using the linear combination of features of $x$, $y$, and $z$.\n",
    "* Like MLPs, transformers are universal function approximators, so they can learn any function $y = f(X)$, but the components/bases of $f$ will be difficult to interpret as there is a vast parameter space\n",
    "\n",
    "Practically, suppose we want to predict $Y = [ \\dot{x}, \\dot{y}, \\dot{z} ]$. To train our model, we will try to learn a transformer $f$ that maps $X = [x, y, z]$ to $Y$, i.e., $Y = f(X)$. We can then assess how well the learned $f$ can be used to simulate/reproduce the Lorenz system under new initial conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd0f6e",
   "metadata": {},
   "source": [
    "#### 3.1: Simulate Lorenz time series with specified parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc1bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define initial conditions and parameters and simulate\n",
    "import tutorial_ddd.models\n",
    "\n",
    "\n",
    "initial_conditions = [0.1, 0.0, 0.0]  # Starting point [x, y, z]\n",
    "sigma_val = 10.0\n",
    "rho_val = 28.0\n",
    "beta_val = 8/3\n",
    "delta_t = 0.01\n",
    "total_steps = 20000 # More steps to see the chaotic attractor\n",
    "\n",
    "print(f\"Simulating Lorenz system with initial conditions: {initial_conditions}\")\n",
    "print(f\"Parameters: sigma={sigma_val}, rho={rho_val}, beta={beta_val}\")\n",
    "print(f\"Time step (dt): {delta_t}, Number of steps: {total_steps}\")\n",
    "\n",
    "# Simulate the system\n",
    "lorenz_trajectory, derivatives = tutorial_ddd.lorenz.simulate_lorenz(\n",
    "    initial_conditions,\n",
    "    sigma=sigma_val,\n",
    "    rho=rho_val,\n",
    "    beta=beta_val,\n",
    "    dt=delta_t,\n",
    "    num_steps=total_steps\n",
    ")\n",
    "\n",
    "noise_amplitude = 0\n",
    "noise = np.random.normal(0,noise_amplitude,lorenz_trajectory.shape)\n",
    "lorenz_trajectory = lorenz_trajectory + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2335ba4",
   "metadata": {},
   "source": [
    "#### 3.2: Initialize and train Transformer (one transformer layer/block) on Lorenz data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03780ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Import Pytorch\n",
    "import torch\n",
    "torch.manual_seed(701)\n",
    "\n",
    "## Instantiate MLP with single hidden layer\n",
    "y = derivatives\n",
    "X = lorenz_trajectory.copy()\n",
    "model = tutorial_ddd.models.Transformer(\n",
    "                        input_dim=1, # dimension of input tokens (time series so just 1)\n",
    "                        output_dim=1, # mask pretraining so just 1\n",
    "                        nhead=1,\n",
    "                        nlayers=1,\n",
    "                        embedding_dim=4\n",
    ")\n",
    "\n",
    "# specify loss function (MSE)\n",
    "MSE = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=0.0001, weight_decay=0.01)\n",
    "dataset = torch.utils.data.TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(y).float())\n",
    "batch_size = 512\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "num_epochs = 500\n",
    "dataframe = {}\n",
    "dataframe['Loss'] = []\n",
    "dataframe['Epoch'] = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_X, batch_Y in dataloader:\n",
    "        batch_X = batch_X.unsqueeze(-1)\n",
    "        batch_Y = batch_Y.unsqueeze(-1)\n",
    "        # set gradients to 0 before computing them again\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute predictions\n",
    "        output = model(batch_X)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = MSE(output, batch_Y)\n",
    "\n",
    "        # Backward pass, compute gradients + update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * batch_X.size(0) # compute overall loss\n",
    "    \n",
    "    # Normalize loss per epoch\n",
    "    epoch_loss = epoch_loss / len(dataset)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n",
    "    dataframe['Loss'].append(epoch_loss)\n",
    "    dataframe['Epoch'].append(epoch)\n",
    "\n",
    "import pandas as pd\n",
    "dataframe = pd.DataFrame(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e277c05",
   "metadata": {},
   "source": [
    "#### 2.4: Evaluate model fit: Compute Lorenz system using the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0178f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateEstimatedLorenzWithTransformer(model, x0, y0, z0, num_steps=20000):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    trajectory = torch.zeros(num_steps,3)\n",
    "    trajectory[0] = torch.tensor([x0, y0, z0])\n",
    "    for t in range(num_steps-1):\n",
    "        dxdt = model(trajectory[t].unsqueeze(-1).unsqueeze(0))[0,:,0]\n",
    "        new_val = trajectory[t] + dxdt\n",
    "        trajectory[t+1] = new_val\n",
    "    return trajectory.detach().numpy()\n",
    "\n",
    "model_trajectory = simulateEstimatedLorenzWithTransformer(model, initial_conditions[0], initial_conditions[1], initial_conditions[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b667ba5a",
   "metadata": {},
   "source": [
    "#### 2.5: Compare the original Lorenz system with the learned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc13379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(lorenz_trajectory[:, 0], lorenz_trajectory[:, 1], lorenz_trajectory[:, 2], lw=0.5, alpha=0.8, color='black', label='lorenz')\n",
    "ax.plot(model_trajectory[:, 0], model_trajectory[:, 1], model_trajectory[:, 2], lw=0.5, alpha=0.8, label='model reconstruction -- Transformer')\n",
    "ax.set_xlabel(\"X-axis\")\n",
    "ax.set_ylabel(\"Y-axis\")\n",
    "ax.set_zlabel(\"Z-axis\")\n",
    "ax.set_title(\"Model Reconstruction: Lorenz Attractor\")\n",
    "plt.legend()\n",
    "\n",
    "# You can also plot individual dimensions against time\n",
    "plt.figure(figsize=(12, 3))\n",
    "time_points = np.arange(0, total_steps * delta_t, delta_t)\n",
    "plt.plot(time_points, model_trajectory[:, 0], label='X-Transformer', color=sns.color_palette('Blues')[1])\n",
    "plt.plot(time_points, lorenz_trajectory[:, 0], label='X(t)-original', color=sns.color_palette('Reds')[1])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"X Variables Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "time_points = np.arange(0, total_steps * delta_t, delta_t)\n",
    "plt.plot(time_points, model_trajectory[:, 1], label='Y-Transformer', color=sns.color_palette('Blues')[1])\n",
    "plt.plot(time_points, lorenz_trajectory[:, 1], label='Y(t)-original', color=sns.color_palette('Reds')[1])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Y Variables Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "time_points = np.arange(0, total_steps * delta_t, delta_t)\n",
    "plt.plot(time_points, model_trajectory[:, 2], label='Z-Transformer', color=sns.color_palette('Blues')[1])\n",
    "plt.plot(time_points, lorenz_trajectory[:, 2], label='Z(t)-original', color=sns.color_palette('Reds')[1])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Z Variables Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_ddd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
